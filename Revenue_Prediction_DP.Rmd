---
title: "Revenue_Prediction"
author: "David Palazzo"
date: "6/7/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(xgboost)
library(dplyr) 
library(naniar)
library(jsonlite)
library(reshape)
```

```{r}
# Load data
test_data = read.csv('test_data.csv')
train_data = read.csv('train_data.csv')
# drop the first and second column of each dataframe (index)
train_data <- train_data[-c(1,3)]
test_data <- test_data[-c(1,3)]
```

```{r}
# Split into X,y
y_train <- train_data$totals.transactionRevenue
X_train <- train_data[, !(colnames(train_data) %in% c('totals.transactionRevenue'))]
y_test <- test_data$totals.transactionRevenue
X_test <- test_data[, !(colnames(test_data) %in% c('totals.transactionRevenue'))]
```

```{r}
#change missing values to 0
X_train$totals.totalTransactionRevenue[is.na(X_train$totals.totalTransactionRevenue)] <- 0
X_train$totals.transactions[is.na(X_train$totals.transactions)] <- 0
X_test$totals.totalTransactionRevenue[is.na(X_test$totals.totalTransactionRevenue)] <- 0
X_test$totals.transactions[is.na(X_test$totals.transactions)] <- 0
#drop missing columns
drop_names <- c("trafficSource.adwordsClickInfo.criteriaParameters","geoNetwork.networkLocation","geoNetwork.longitude","geoNetwork.latitude","geoNetwork.cityId","device.screenResolution","device.screenColors","device.operatingSystemVersion", "device.mobileInputSelector","device.mobileDeviceModel","device.mobileDeviceMarketingName","device.mobileDeviceInfo","device.mobileDeviceBranding","device.language","device.flashVersion","device.browserVersion","device.browserSize")

X_train <- X_train[,!(names(X_train) %in% drop_names)]
X_test <- X_test[,!(names(X_test) %in% drop_names)]
```

```{r}
# Drop ID columns and total Rev
X_train %<>% 
  select(-date, -fullVisitorId, -visitId, -visitStartTime, -totals.totalTransactionRevenue, -totals.transactions) %>% 
  mutate_if(is.character, funs(factor(.) %>% as.integer))
X_test  %<>% 
  select(-date, -fullVisitorId, -visitId, -visitStartTime,  -totals.totalTransactionRevenue,-totals.transactions) %>% 
  mutate_if(is.character, funs(factor(.) %>% as.integer))
# Replace NAs with 0 for y variable revenue
y_train[is.na(y_train)]<- 0
y_test[is.na(y_test)] <- 0
```


For EDA purposes we will transform y variable to actual dollar value. Our data has transaction revenue multiplied by $10^6$. 

```{r}
train_y_EDA<- y_train/1000000
summary(train_y_EDA)
# Drop 0 instances
train_y_EDA<- train_y_EDA[!(train_y_EDA==0)]
# drop values over 1000
train_y_EDA<- train_y_EDA[!(train_y_EDA>1000)]
g <- ggplot(data = data_frame(train_y_EDA),aes(x = train_y_EDA)) + 
geom_histogram(binwidth = 10,color='black', fill='blue') + labs(title = "Histogram of Transaction Revenue") + labs(x=" Transaction Revenue in Dollars")
g
```

Let's take a look at the density plot of the natural log of Transaction revenue, which is the y value that we will predict.

```{r}
train_y_EDA<- log(train_y_EDA)
g <- ggplot(data = data_frame(train_y_EDA),aes(x=train_y_EDA, y= ..density..)) + 
geom_histogram(binwidth = .6, color='black',fill='blue') + labs(title = "Density of Log Transaction Revenue") + labs(x=" Log Transaction Revenue") 
g
```
```{r}
train_y_copy = y_train
bool=ifelse(y_train==0,'No','Yes')
g <- ggplot(data = data_frame(bool),aes(x=as.factor(bool))) + 
geom_bar(color='black',fill='blue') + labs(title = "Visits that Resulted in Revenue") + labs(x="Did a visit result in revenue?") 
g
bool1=ifelse(y_train==0,0,1)
print(sum(bool1)/length(bool1))
```


We are predicting the log of Transaction revenue, let's transform our y variable. 

```{r}
y_test <- log1p(y_test)
y_train <- log1p(y_train)
```


```{r}
dtrain <- xgb.DMatrix(data = data.matrix(X_train),label = data.matrix(y_train))
dtest <- xgb.DMatrix(data = data.matrix(X_test),label=data.matrix(y_test))
```
We'll use cross validation in order to find the optimal max number of boosting iterations.

```{r}
set.seed(123456)
xgbcv <- xgb.cv(data = dtrain, nrounds = 100, nfold = 5, stratified = T,early_stopping_rounds = 20)
nrounds <- xgbcv$best_iteration
#y_pred <- predict(xgb, dtest)
```

```{r}
#xgb <- xgb.train(data = dtrain, nrounds = nrounds,watchlist = list(val=dtest, train=dtrain))
#xgb$evaluation_log$val_rmse
```




```{r}
#X_names <- names(X_train)
#importance_matrix <- xgb.importance(X_names, model = xgb)
# Nice graph
#xgb.plot.importance(importance_matrix)

```

```{r}
#ETA
set.seed(1)
eta=c(0.05,0.1,0.2,0.5,1)
conv_eta = matrix(NA,500,length(eta))
pred_eta = matrix(NA,500,length(eta))
colnames(conv_eta) = colnames(pred_eta) = eta
for(i in 1:length(eta)){
  params=list(eta = eta[i], colsample_bylevel=2/3,
              subsample = 0.5, max_depth = 6,
              min_child_weigth = 1)
  xgb <- xgb.train(data = dtrain, nrounds = 500 , params = params, watchlist = list(val=dtest, train=dtrain))
  conv_eta[,i] = xgb$evaluation_log$train_rmse
  pred_eta[,i] = xgb$evaluation_log$val_rmse
}

conv_eta = data.frame(iter=1:500, conv_eta)
conv_eta = melt(conv_eta,id.vars = "iter")

g<- ggplot(data = conv_eta) + geom_line(aes(x = iter, y = value, color = variable)) + labs(title = "Training Set Effect of Learning parameter", y = "RMSE", x= "Number of Iterations")
print(g)

pred_eta = data.frame(iter=1:500, pred_eta)
pred_eta = melt(pred_eta,id.vars = "iter")

gg<- ggplot(data = pred_eta) + geom_line(aes(x = iter, y = value, color = variable)) + labs(title = "Test Set Effect of Learning parameter", y = "RMSE", x= "Number of Iterations")
print(gg)
```



```{r}
# Max Depth
md=c(2,4,6,10)
conv_md = matrix(NA,50,length(md))
pred_md = matrix(NA,50,length(md))
colnames(conv_md) = colnames(pred_md) = md
for(i in 1:length(md)){
  params=list(eta=0.1,colsample_bylevel=2/3,
              subsample=0.5,max_depth=md[i],
              min_child_weigth=1)
  xgb <- xgb.train(data = dtrain, nrounds = 50 , params = params, watchlist = list(val=dtest, train=dtrain))
  conv_md[,i] = xgb$evaluation_log$train_rmse
  pred_md[,i] = xgb$evaluation_log$val_rmse
}

conv_md = data.frame(iter=1:50, conv_md)
conv_md = melt(conv_md, id.vars = "iter")
pred_md = data.frame(iter=1:50, pred_md)
pred_md = melt(pred_md, id.vars = "iter")

g <- ggplot(data = conv_md) + geom_line(aes(x = iter, y = value, color = variable)) + labs(title = "Training Set Error Max Depth", y = "RMSE", x= "Number of Iterations")
print(g)
gg<- ggplot(data = pred_md) + geom_line(aes(x = iter, y = value, color = variable)) + labs(title = "Validation Set Error Max Depth", y = "RMSE", x= "Number of Iterations")
print(gg)

```

```{r}
# Gamma 
set.seed(1)
gamma=c(0.1,1,10,100)
conv_gamma = matrix(NA,500,length(gamma))
pred_gamma = matrix(NA,nrow(X_test), length(gamma))
colnames(conv_gamma) = colnames(pred_gamma) = gamma
for(i in 1:length(gamma)){
  params = list(eta = 0.1, colsample_bylevel=2/3,
              subsample = 1, max_depth = 6, min_child_weight = 1,
              gamma = gamma[i])
  xgb = xgboost(dtrain, nrounds = 500, params = params)
  conv_gamma[,i] = xgb$evaluation_log$train_rmse
  pred_gamma[,i] = predict(xgb, dtest)
}
 
conv_gamma = data.frame(iter=1:500, conv_gamma)
conv_gamma = melt(conv_gamma, id.vars = "iter")
ggplot(data = conv_gamma) + geom_line(aes(x = iter, y = value, color = variable))+ labs(title = "Gamma Values", y = "RMSE", x= "Number of Iterations")
(RMSE_gamma = sqrt(colMeans((y_test-pred_gamma)^2)))
pred_gamma
```

