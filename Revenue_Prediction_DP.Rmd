---
title: "Revenue_Prediction"
author: "David Palazzo"
date: "6/7/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(xgboost)
library(dplyr) 
library(naniar)
library(jsonlite)
library(reshape)
library(mlr)
```

```{r}
# Load data
test_data = read.csv('test_data.csv')
train_data = read.csv('train_data.csv')
# drop the first and second column of each dataframe (index)
train_data <- train_data[-c(1,3)]
test_data <- test_data[-c(1,3)]
```

```{r}
# Split into X,y
y_train <- train_data$totals.transactionRevenue
X_train <- train_data[, !(colnames(train_data) %in% c('totals.transactionRevenue'))]
y_test <- test_data$totals.transactionRevenue
X_test <- test_data[, !(colnames(test_data) %in% c('totals.transactionRevenue'))]
# Split index for validation data
split_idx <- X_train$date < 20160808
```

```{r}
#change missing values to 0
X_train$totals.totalTransactionRevenue[is.na(X_train$totals.totalTransactionRevenue)] <- 0
X_train$totals.transactions[is.na(X_train$totals.transactions)] <- 0
X_test$totals.totalTransactionRevenue[is.na(X_test$totals.totalTransactionRevenue)] <- 0
X_test$totals.transactions[is.na(X_test$totals.transactions)] <- 0
#drop missing columns
drop_names <- c("trafficSource.adwordsClickInfo.criteriaParameters","geoNetwork.networkLocation","geoNetwork.longitude","geoNetwork.latitude","geoNetwork.cityId","device.screenResolution","device.screenColors","device.operatingSystemVersion", "device.mobileInputSelector","device.mobileDeviceModel","device.mobileDeviceMarketingName","device.mobileDeviceInfo","device.mobileDeviceBranding","device.language","device.flashVersion","device.browserVersion","device.browserSize")

X_train <- X_train[,!(names(X_train) %in% drop_names)]
X_test <- X_test[,!(names(X_test) %in% drop_names)]
```

```{r}
# Drop ID columns and total Rev
X_train <- X_train %<>% 
  select(-date, -fullVisitorId, -visitId, -visitStartTime, -totals.totalTransactionRevenue, -totals.transactions) %>% 
  mutate_if(is.character, funs(factor(.) %>% as.integer))
X_test <- X_test  %<>% 
  select(-date, -fullVisitorId, -visitId, -visitStartTime,  -totals.totalTransactionRevenue,-totals.transactions) %>% 
  mutate_if(is.character, funs(factor(.) %>% as.integer))
# Replace NAs with 0 for y variable revenue
y_train[is.na(y_train)]<- 0
y_test[is.na(y_test)] <- 0
```


For EDA purposes we will transform y variable to actual dollar value. Our data has transaction revenue multiplied by $10^6$. 

```{r}
train_y_EDA<- y_train/1000000
summary(train_y_EDA)
# Drop 0 instances
train_y_EDA<- train_y_EDA[!(train_y_EDA==0)]
# drop values over 1000
train_y_EDA<- train_y_EDA[!(train_y_EDA>1000)]
g <- ggplot(data = data_frame(train_y_EDA),aes(x = train_y_EDA)) + 
geom_histogram(binwidth = 10,color='black', fill='blue') + labs(title = "Histogram of Transaction Revenue") + labs(x=" Transaction Revenue in Dollars")
g
```

Let's take a look at the density plot of the natural log of Transaction revenue, which is the y value that we will predict.

```{r}
train_y_EDA<- log(train_y_EDA)
g <- ggplot(data = data_frame(train_y_EDA),aes(x=train_y_EDA, y= ..density..)) + 
geom_histogram(binwidth = .6, color='black',fill='blue') + labs(title = "Density of Log Transaction Revenue") + labs(x=" Log Transaction Revenue") 
g
```
```{r}
train_y_copy = y_train
bool=ifelse(y_train==0,'No','Yes')
g <- ggplot(data = data_frame(bool),aes(x=as.factor(bool))) + 
geom_bar(color='black',fill='blue') + labs(title = "Visits that Resulted in Revenue") + labs(x="Did a visit result in revenue?") 
g
bool1=ifelse(y_train==0,0,1)
print(sum(bool1)/length(bool1))
```


We are predicting the log of Transaction revenue, let's transform our y variable. 

```{r}
y_test <- log1p(y_test)
y_train <- log1p(y_train)
```


```{r}
dtrain <- xgb.DMatrix(data = data.matrix(X_train[split_idx, ]), label = data.matrix(y_train[split_idx ]))
dval <- xgb.DMatrix(data = data.matrix(X_train[!split_idx, ]), label = data.matrix(y_train[!split_idx ]))
dtest <- xgb.DMatrix(data = data.matrix(X_test),label=data.matrix(y_test))
```
We'll use cross validation in order to find the optimal max number of boosting iterations.


```{r}
xgb <- xgb.train(data = dtrain, nrounds = nrounds,watchlist = list(val=dtest, train=dtrain))
xgb$evaluation_log$val_rmse
```


```{r}
#X_names <- names(X_train)
#importance_matrix <- xgb.importance(X_names, model = xgb)
# Nice graph
#xgb.plot.importance(importance_matrix)

```

```{r}
#ETA
set.seed(1)
eta=c(0.05,0.1,0.2,0.5,1)
train_eta = matrix(NA,100,length(eta))
val_eta = matrix(NA,100,length(eta))
colnames(train_eta) = colnames(val_eta) = eta

for(i in 1:length(eta)){
  params=list(eta = eta[i], colsample_bylevel=2/3,
              subsample = 0.5, max_depth = 6,
              min_child_weight = 1)
  xgb <- xgb.train(data = dtrain, nrounds = 100 , params = params, watchlist = list(val=dval, train=dtrain), verbose = 0)
  train_eta[,i] = xgb$evaluation_log$train_rmse
  val_eta[,i] = xgb$evaluation_log$val_rmse
}

train_eta = data.frame(iter=1:100, train_eta)
train_eta = melt(train_eta,id.vars = "iter")

g<- ggplot(data = train_eta) + geom_line(aes(x = iter, y = value, color = variable)) + labs(title = "Training Set Effect of Learning parameter", y = "RMSE", x= "Number of Iterations")
print(g)

val_eta = data.frame(iter=1:100, val_eta)
val_eta = melt(val_eta,id.vars = "iter")

gg<- ggplot(data = val_eta) + geom_line(aes(x = iter, y = value, color = variable)) + labs(title = "Validation Set Effect of Learning parameter", y = "RMSE", x= "Number of Iterations")
print(gg)
```



```{r}
# Max Depth
md=c(2,4,6,10)
train_md = matrix(NA,100,length(md))
val_md = matrix(NA,100,length(md))
colnames(train_md) = colnames(val_md) = md
for(i in 1:length(md)){
  params=list(eta=0.1,colsample_bylevel=2/3,
              subsample=0.5,max_depth=md[i],
              min_child_weight=1)
  xgb <- xgb.train(data = dtrain, nrounds = 100 , params = params, watchlist = list(val=dval, train=dtrain), verbose = 0)
  train_md[,i] = xgb$evaluation_log$train_rmse
  val_md[,i] = xgb$evaluation_log$val_rmse
}

train_md = data.frame(iter=1:100, train_md)
train_md = melt(train_md, id.vars = "iter")
val_md = data.frame(iter=1:100, val_md)
val_md = melt(val_md, id.vars = "iter")

g <- ggplot(data = train_md) + geom_line(aes(x = iter, y = value, color = variable)) + labs(title = "Training Set Error Max Depth", y = "RMSE", x= "Number of Iterations")
print(g)
gg<- ggplot(data = val_md) + geom_line(aes(x = iter, y = value, color = variable)) + labs(title = "Validation Set Error Max Depth", y = "RMSE", x= "Number of Iterations")
print(gg)

```

```{r}
# Gamma 
set.seed(1)
gamma=c(0.1,1,10,100)
train_gamma = matrix(NA,100,length(gamma))
val_gamma = matrix(NA,100,length(gamma))
colnames(train_gamma) = colnames(val_gamma) = gamma
for(i in 1:length(gamma)){
  params = list(eta = 0.1, colsample_bylevel=2/3,
              subsample = 1, max_depth = 6, min_child_weight = 1,
              gamma = gamma[i])
  xgb <- xgb.train(data = dtrain, nrounds = 100 , params = params, watchlist = list(val=dval, train=dtrain), verbose = 0)
  train_gamma[,i] = xgb$evaluation_log$train_rmse
  val_gamma[,i] = xgb$evaluation_log$val_rmse
}
 
train_gamma = data.frame(iter=1:100, train_gamma)
train_gamma = melt(train_gamma, id.vars = "iter")
ggplot(data = train_gamma) + geom_line(aes(x = iter, y = value, color = variable))+ labs(title = "Training Set Gamma Values", y = "RMSE", x= "Number of Iterations")

val_gamma = data.frame(iter=1:100, val_gamma)
val_gamma = melt(val_gamma, id.vars = "iter")
ggplot(data = val_gamma) + geom_line(aes(x = iter, y = value, color = variable))+ labs(title = "Gamma Value Error Validation Set", y = "RMSE", x= "Number of Iterations")

```

We'll now use the mlr library to do hyperparameter tunning. 

```{r}
params <- makeParamSet(makeDiscreteParam("booster",values = c("gbtree","gblinear")), makeIntegerParam("max_depth",lower = 3L,upper = 10L), makeNumericParam("min_child_weight",lower = 1L,upper = 10L), makeNumericParam("subsample",lower = 0.5,upper = 1),makeIntegerParam("nrounds",lower = 2L,upper = 20L) ,makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))

X_train <- X_train %>%mutate_if(is.factor, as.integer)

trainDf <- X_train[split_idx, ]
trainDf$target = y_train[split_idx ]

traintask <- makeRegrTask (data = data.frame(trainDf),target = 'target')

valDf <- X_train[!split_idx, ]
valDf$target = y_train[!split_idx ]

testtask <- makeRegrTask(data = data.frame(valDf),target = 'target')

lrn <- makeLearner("regr.xgboost")
lrn$par.vals <- list( objective='reg:linear', nrounds=100L, eta=0.05)
rdesc <- makeResampleDesc("CV",iters=5L)

ctrl <- makeTuneControlRandom(maxit = 10L)

mytune <- tuneParams(learner = lrn, task = traintask,resampling=rdesc, par.set = params, control = ctrl, show.info = T)
mytune
```

Now that we have tunned the parameters to the model, let's train the final model and see how it performs on the test set.
```{r}
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
# Use both train and validation sets to train final model 
dtrain <- xgb.DMatrix(data = data.matrix(X_train), label = data.matrix(y_train))
xgb <- xgb.train(data = dtrain,nrounds = 7,lrn_tune=lrn_tune,watchlist = list(val=dtest, train=dtrain))
```
```{r}
X_names <- names(X_train)
importance_matrix <- xgb.importance(X_names, model = xgb)
xgb.plot.importance(importance_matrix)
```



